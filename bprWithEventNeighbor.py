# coding=utf-8

import numpy as np
from math import log, exp
from event_neighbor_score_version import ItemNeighbor
from real_social_influence_order_finder import get_real_social_influence_order       #改动1
from getdata import DataMode
import random

"""
    @author:    zhaomingxing
    @update:    2016.8.3
"""

"""
Bayesian Personalized Ranking with Neighbor
Matrix Factorization model and a variety of classes
implementing different sampling strategies.

Original data is a csr_matrix of <event, user>
After training the data we get event_factors(W), user_factors(H) and user_bias
"""


class BPRArgs(object):

    def __init__(self, learning_rate=0.1,     #0.01      
                 bias_regularization=1.0,
                 event_regularization=0.0025,
                 positive_user_regularization=0.0025,
                 negative_user_regularization=0.00025,
                 update_negative_user_factors=True):
        self.learning_rate = learning_rate
        self.bias_regularization = bias_regularization
        self.event_regularization = event_regularization
        self.positive_user_regularization = positive_user_regularization
        self.negative_user_regularization = negative_user_regularization
        self.update_negative_user_factors = update_negative_user_factors


class BPR(object):

    def __init__(self, dimension, arg, e_neighbors, t_data):
        """initialise BPR matrix factorization model
        D: number of factors
        args: parameter table
        neighbors are dict generated by find_neighbors
        """
        self.dimension = dimension
        self.learning_rate = arg.learning_rate
        self.bias_regularization = arg.bias_regularization
        self.event_regularization = arg.event_regularization
        self.positive_user_regularization = arg.positive_user_regularization
        self.negative_user_regularization = arg.negative_user_regularization
        self.update_negative_user_factors = arg.update_negative_user_factors
        self.events_neighbors = e_neighbors
        self.iter_num_x = []
        self.data = None
        self.user_factors = None
        self.user_bias = None
        self.event_factors = None
        self.loss_samples = None
        self.num_users = None
        self.num_events = None
        self.test_data = t_data

    def train(self, data, sampler, iters_num):
        """train model
        data: event-user matrix as a scipy sparse matrix
              events and users are zero-indexed
              here convergence is replaced by iteration number limit(5000)
        """
        self.init(data)
        # initial loss is random
        print('initial loss = {0}'.format(self.loss()))

        # TODO: Update factors(W and H) and show negative BPR-OPT during each iteration
        # TODO: Show negative BPR-OPT to make sure that we are minimizing it
        for it in range(iters_num):
            self.iter_num_x.append(it)
            print('starting iteration {0}'.format(it))
            for e, i, j in sampler.generate_samples(self.data):
                self.update_factors(e, i, j)
            print('iteration {0}: loss = {1}'.format(it, self.loss()))

        with open('event_factor_file.txt', 'w+') as event_factor_file:
            for event_factor in self.event_factors:
                for i in range(self.dimension):
                    event_factor_file.write(str(event_factor[i]) + ' ')
                event_factor_file.write('\n')
        event_factor_file.close()

        with open('user_factor_file.txt', 'w+') as user_factor_file:
            for user_factor in self.user_factors:
                for i in range(self.dimension):
                    user_factor_file.write(str(user_factor[i]) + ' ')
                user_factor_file.write('\n')
        user_factor_file.close()

        with open('bias_file.txt', 'w+') as bias_file:
            for bias in self.user_bias:
                bias_file.write(str(bias) + '\n')
        bias_file.close()

    def init(self, data):
        self.data = data
        # data is csr matrix
        self.num_events, self.num_users = self.data.shape
        print self.num_users
        # user_bias is an one-dimension array, its length = num_users, all elements are initialized to 0
        self.user_bias = np.zeros(self.num_users)
        # event_factors is self.num_events-by-self.D array of random values
        # it equals the W(factor matrix of events) in the paper
        # TODO: initialise the event factor matrix W with random values
        self.event_factors = np.random.randn(self.num_events, self.dimension) * 0.5
        # user_factors is self.num_users-by-self.D(number of factors, 10) array of random values
        # it equals the H(factor matrix of users) in the paper
        # TODO: initialise the user factor matrix H with random values
        self.user_factors = np.random.randn(self.num_users, self.dimension) * 0.5
        self.create_loss_samples()

    def create_loss_samples(self):
        # apply rule of thumb to decide num samples over which to compute loss
        num_loss_samples = int(100*self.num_events**0.5)
        print('sampling {0} <event, user i, user j> triples...'.format(num_loss_samples))
        # construct a sampler object named sampler
        sampler = UniformEventUniformUser(True)
        """TODO:compare num generated by rule of thumb with num of items of train_data
        the smaller one is the length of the list of triples(e,i,j) generated by sampler
        DS is a triple array and it shows all the information of the data
        data generated by sampler is just a part of DS and may has repetition in it
        """
        self.loss_samples = [t for t in sampler.generate_samples(self.data, num_loss_samples)]

    def update_factors(self, e, i, j, update_e=True, update_i=True):
        """apply SGD update"""

        update_j = self.update_negative_user_factors

        e_factor_with_neighbor = self.event_factors[e]
        for e_neighbor in self.events_neighbors[e]:
            e_factor_with_neighbor += self.event_factors[e_neighbor]
        e_factor_with_neighbor /= (1 + len(self.events_neighbors[e]))

        x = self.user_bias[i] - self.user_bias[j] \
            + np.dot(e_factor_with_neighbor, self.user_factors[i,:] - self.user_factors[j,:])
        z = 1.0 / (1.0 + exp(x))

        if update_i:
            d = -self.bias_regularization * self.user_bias[i] + z
            self.user_bias[i] += self.learning_rate * d
        if update_j:
            d = - self.bias_regularization * self.user_bias[j] - z
            self.user_bias[j] += self.learning_rate * d
        if update_e:
            d = -self.event_regularization * self.event_factors[e, :]  + \
                (self.user_factors[i,:] - self.user_factors[j,:]) * z / (1 + len(self.events_neighbors[e]))
            self.event_factors[e, :] += self.learning_rate * d
        if update_i:
            d = -self.positive_user_regularization * \
                self.user_factors[i, :] + e_factor_with_neighbor * z
            self.user_factors[i, :] += self.learning_rate * d
        if update_j:
            d = -self.negative_user_regularization * \
                self.user_factors[j, :] - e_factor_with_neighbor * z
            self.user_factors[j, :] += self.learning_rate * d
        #update pl  认为pe和pl地位是一样的，所以也要update
        for e_neighbor in self.events_neighbors[e]:
            d = -self.event_regularization * self.event_factors[e_neighbor, :]  + \
                (self.user_factors[i,:] - self.user_factors[j,:]) * z / (1 + len(self.events_neighbors[e]))
            self.event_factors[e_neighbor, :] += self.learning_rate * d

    def loss(self):
        ranking_loss = 0
        for e, i, j in self.loss_samples:
            x = self.predict(e, i) - self.predict(e, j)
            ranking_loss += -log(1.0 / (1.0 + exp(-x)))
        complexity = 0
        for e, i, j in self.loss_samples:
            complexity += self.event_regularization * np.dot(self.event_factors[e], self.event_factors[e])
            for e_neighbor in self.events_neighbors[e]:  #这里改成pe的正则化因子和pl的加一起之后要平均，就是(lamda*pe^2+xigma lamda*pl^2)/(1+len(Ne)
                complexity += self.event_regularization * np.dot(self.event_factors[e_neighbor], self.event_factors[e_neighbor])
            complexity /= (1 + len(self.events_neighbors[e]))
            complexity += self.positive_user_regularization * np.dot(self.user_factors[i], self.user_factors[i])
            complexity += self.negative_user_regularization * np.dot(self.user_factors[j], self.user_factors[j])
            complexity += self.bias_regularization * self.user_bias[i]**2
            complexity += self.bias_regularization * self.user_bias[j]**2
            
        return ranking_loss + complexity
        # return negative BPR-OPT

    def predict(self, e, i):
        # get event e's preference for user i
        e_factor_with_neighbor = self.event_factors[e]
        for e_neighbor in self.events_neighbors[e]:
            e_factor_with_neighbor += self.event_factors[e_neighbor]
        e_factor_with_neighbor /= (1 + len(self.events_neighbors[e]))

        return self.user_bias[i] + np.dot(e_factor_with_neighbor, self.user_factors[i])

    # TODO: calculate AUC by factors and test_data



    #逻辑问题
    def calculate_auc(self):
        sum2 = 0
        u_sum = 0
        for piece in self.test_data:
            event_id = piece[0]
            user_id = piece[1]
            index = self.data[event_id].indices
            positive_user_list = list(index)
            positive_user_list.append(user_id)
            negative_user_list = list(set(range(self.num_users)) - set(positive_user_list))
            x_u_i = np.dot(self.event_factors[event_id], self.user_factors[user_id]) + self.user_bias[user_id]    #test_data only one
            for j in negative_user_list:  
                x_u_j = np.dot(self.event_factors[event_id], self.user_factors[j]) + self.user_bias[j]
                if x_u_i > x_u_j:
                    u_sum += 1.0
            sum2+=len(negative_user_list)
        auc = u_sum/sum2

        return auc

    #逻辑问题
    def calculate_map(self):
        event_sum = 0
        real_order = get_real_social_influence_order()
        for piece in self.test_data:
            event_id = piece[0]
            user_list = range(self.num_users)

            user_influence_dict = {}
            for user in user_list:
                user_influence_dict[user] = np.dot(self.event_factors[event_id], self.user_factors[user]) \
                                            + self.user_bias[user]
            sorted_users_by_influence = sorted(user_influence_dict.items(), key=lambda d: d[1], reverse=True)
            sorted_user_list = []
            for i in sorted_users_by_influence:
                sorted_user_list.append(i[0])

            # calculate p@n
            limit = 10
            if len(real_order[event_id]) < 10:
                limit = len(real_order[event_id])

            predict_n = 0
            valid_n = 0
            for i in range(limit):
                counter = -1
                for j in range(limit):
                    if real_order[event_id][i] == sorted_user_list[j]:
                        counter = j
                        break
                if counter != -1:
                    valid_n += 1.0
                    predict_n += 1.0 / (1.0 + counter)
            if valid_n != 0:
                predict_n /= valid_n
            event_sum += predict_n
        map_mark = event_sum / len(self.test_data)
        return map_mark


class Sampler(object):
    def __init__(self, sample_negative_users_empirically):
        self.sample_negative_users_empirically = sample_negative_users_empirically
        self.data = None
        self.num_events = None
        self.num_users = None
        self.max_samples = None

    def init(self, data, max_samples=None):
        self.data = data
        self.num_events, self.num_users = data.shape
        self.max_samples = max_samples

    def sample_event(self):
        e = self.uniform_event()
        num_users = self.data[e].getnnz()
        assert(num_users > 0 and num_users != self.num_users)
        return e

    def sample_negative_user(self, event_users):
        j = self.random_user()
        while j in event_users:
            j = self.random_user()
        return j

    def uniform_event(self):
        return random.randint(0, self.num_events-1)

    def random_user(self):
        """sample an item uniformly or from the empirical distribution
           observed in the training data
        """
        if self.sample_negative_users_empirically:
            # just pick something someone rated!
            #  TODO: choose a user randomly
            e = self.uniform_event()
            i = random.choice(self.data[e].indices)
        else:
            i = random.randint(0, self.num_users - 1)
        return i

    def num_samples(self, n):
        if self.max_samples is None:
            return n
        return min(n, self.max_samples)


# TODO:sample users and items separately
# this sample strategy includes negative feedback
class UniformEventUniformUser(Sampler):
    def generate_samples(self, data, max_samples=None):
        self.init(data, max_samples)
        """nnz is the number of nonzero values in sparse matrix
        the matrix should have been all positive elements
        and in num_samples() we compare data.nnz with num_loss_samples to identify num_samples
        """
        for _ in range(self.num_samples(self.data.nnz)):
            # TODO: choose a user randomly
            e = self.uniform_event()
            i = random.choice(self.data[e].indices)
            # TODO: sample negative item
            j = self.sample_negative_user(self.data[e].indices)
            yield e, i, j


# idxs make it sure that there is not repetition
class UniformPairWithoutReplacement(Sampler):
    # train called generate_samples with only one parameter(data:a csr matrix)
    def generate_samples(self, data, max_samples=None):
        self.init(data, max_samples)
        self.events, self.users = self.data.nonzero()
        idxs = list(range(self.events.size))
        # idxs is a list from 0 to data.nonzero().size-1
        """TODO: make idxs(a list) out of order
        thus make positive users and items out of order
        """
        random.shuffle(idxs)
        self.events = self.events[idxs]
        self.users = self.users[idxs]
        self.idx = 0
        # get users.size samples at last
        for _ in range(self.num_samples(self.events.size)):
            e = self.events[self.idx]
            i = self.users[self.idx]
            j = self.sample_negative_user(self.data[e].indices)
            self.idx += 1
            yield e, i, j

if __name__ == '__main__':
    # TODO: generate data
    d = DataMode()
    d.find_data()
    train_data = d.find_train_data()
    # train_data in csr format is mainly ordered by row_number
    test_data = d.find_test_data()
    # test_data is a list and its element is a tuple of event_id and positive user_id
    args = BPRArgs()
    args.learning_rate = 0.01
    loc_neighbor_num = 100
    neighbor_num_limit = 10
    num_factor = 20
    neighbor_model = ItemNeighbor(neighbor_num_limit)
    items_neighbors = neighbor_model.get_neighbors()

    model = BPR(num_factor, args, items_neighbors, test_data)

    sample_negative_users_empirically = False
    # use a sample strategy without repetition
    sampler = UniformPairWithoutReplacement(sample_negative_users_empirically)

     #iter_num = 50
    iter_num = 5000
    model.train(train_data, sampler, iter_num)

    with open('auc.txt', 'w+') as f:
        f.write(str(model.calculate_auc()) + '\n')

    with open('map.txt', 'w') as f:
        f.write(str(model.calculate_map()) + '\n')
